{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD : Single Shot MultiBox Detector\n",
    "\n",
    "pytorch version of https://gluon.mxnet.io/chapter08_computer-vision/object-detection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default anchor boxes\n",
    "\n",
    "+ image shape : 3,w,h\n",
    "+ sizes : [s1, s2, s3, ... , $s_n$]\n",
    "+ ratios : [r1, r2, r3, ... , $r_n$]\n",
    "+ anchor box \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxPrior(x ,sizes=[.5, .25, .1], ratios=[1, 2, .5]):\n",
    "    sizes = torch.Tensor(sizes)\n",
    "    ratios = torch.Tensor(ratios).sqrt()\n",
    "    #x : [C,W,H]\n",
    "    b,w,h = x.data.size()[0],x.data.size()[2],x.data.size()[3]\n",
    "    n,m = sizes.data.size()[0],ratios.data.size()[0]\n",
    "    num_priors = n+m-1\n",
    "    priors = torch.zeros(w, h, num_priors,4)\n",
    "    w_ = torch.cat((sizes*ratios[0],ratios[1:]*sizes[0]),0)\n",
    "    h_ = torch.cat((sizes/ratios[0],1./ratios[1:]*sizes[0]),0)\n",
    "    for j in range(w):\n",
    "        for k in range(h):\n",
    "            x_c, y_c = torch.tensor((j+1)/w).expand(num_priors), torch.tensor((1+k)/h).expand(num_priors)\n",
    "            priors[j][k]  = torch.cat(((x_c[0]-w_/2).view(num_priors,-1),(y_c[0]-h_/2).view(num_priors,-1),\n",
    "                                       (x_c[0]+w_/2).view(num_priors,-1),(y_c[0]+h_/2).view(num_priors,-1)),1)\n",
    "    \n",
    "    #dims = [b,w,h,num_priors,4]\n",
    "    #priors = priors.expand(tuple(dims))\n",
    "    return priors.view(b,-1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1978590488433838\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "n = 32\n",
    "x = torch.randn(1,3,n,n)\n",
    "\n",
    "start = time.time()\n",
    "boxes = MultiBoxPrior(x)\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = boxes.view((n, n, 5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADOxJREFUeJzt3WGoXGV+x/Hvr1bbsgqrzW0IUZvdrbT4ohvlEiwry3a3\nLtY3KpSiL7a+ELKUFRS2L2QLVaEv3FKVviiWWGVDsVpbFUORdlMRZGFxvdoYo2mrK1nWEJMr7qJ9\n063674s5KZNs7r2TO2fOJPf5fmCYM2fO3PPnSX5znnPmnOekqpDUnl+YdwGS5sPwS40y/FKjDL/U\nKMMvNcrwS40y/FKjDL/UKMMvNeoXp/lwkmuBvwLOAf62qu5dbflNmzbVtm3bplmlpFUcOnSI9957\nL5Msu+7wJzkH+GvgGuAd4KUke6rqjZU+s23bNpaWlta7SklrWFxcnHjZabr9O4C3qurtqvoZ8Dhw\n/RR/T9KApgn/VuDHY6/f6eZJOgvM/IBfkp1JlpIsLS8vz3p1kiY0TfgPA5eMvb64m3eCqtpVVYtV\ntbiwsDDF6iT1aZrwvwRcluQzSc4DbgL29FOWpFlb99H+qvooyW3AvzL6qe+Rqnq9t8okzdRUv/NX\n1bPAsz3VImlAnuEnNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMv\nNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNWqqO/YkOQR8CHwM\nfFRVi30UJWn2pgp/53er6r0e/o6kAdntlxo1bfgL+G6Sl5Ps7KMgScOYttt/dVUdTvJrwN4k/1FV\nL4wv0H0p7AS49NJLp1ydpL5MteWvqsPd8zHgaWDHKZbZVVWLVbW4sLAwzeok9Wjd4U/yqSQXHJ8G\nvgoc6KswSbM1Tbd/M/B0kuN/5++r6l96qUo/L/Mu4CxX8y7gzLPu8FfV28Dne6xF0oD8qU9qVB8n\n+WhodmEn467SqtzyS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMMvNcrw\nS40y/FKjDL/UKMMvNcrwS40y/FKjDL/UKMfwG1CmGFNufNi+af5OS/pss9qA4ya65ZcaZfilRtnt\nn5PT7kaOdVs3Yhd0JqZss42+e7Xmlj/JI0mOJTkwNu+iJHuTvNk9XzjbMiX1bZJu/3eAa0+adyfw\nXFVdBjzXvZZ0Flkz/FX1AvD+SbOvB3Z307uBG3quS9KMrfeA3+aqOtJNv8vojr2SziJTH+2vqmKV\nu8cl2ZlkKcnS8vLytKuT1JP1hv9oki0A3fOxlRasql1VtVhViwsLC+tcnaS+rTf8e4BbuulbgGf6\nKUfSUCb5qe8x4PvAbyZ5J8mtwL3ANUneBH6vey3pLLLmST5VdfMKb32l51okDcjTe6VGGX6pUYZf\napQX9pwkA13NcbqrqbFTKYaq8WzXZ5vNsslrTldqueWXGmX4pUbZ7V9F392xTHN9+Qmf9YL+iUzZ\nZlP9e635t+e/6+aWX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRvk7/waVe+b/O/Ks1V2e7zANt/xS\nowy/1Ci7/Q3YSN3jFnZnhuKWX2qU4ZcaZfilRhl+qVGGX2qU4ZcaZfilRk1yu65HkhxLcmBs3t1J\nDifZ1z2um22Zkvo2yZb/O8C1p5j/QFVt7x7P9luWpFlbM/xV9QLw/gC1SBrQNPv8tyXZ3+0WXNhb\nRZIGsd7wPwh8DtgOHAHuW2nBJDuTLCVZWl5eXufqJPVtXeGvqqNV9XFVfQI8BOxYZdldVbVYVYsL\nCwvrrVNDS2b/0FytK/xJtoy9vBE4sNKyks5Ma17Sm+Qx4EvApiTvAHcBX0qyHSjgEPD1GdYoaQbW\nDH9V3XyK2Q/PoBadqfq8V5Xd/TOGZ/hJjTL8UqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8\nUqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqPWHMZLcuitjcktv9Qowy81ym6/Tq3PEXt1\nRnLLLzXK8EuNMvxSo9YMf5JLkjyf5I0krye5vZt/UZK9Sd7snr1Nt3QWmWTL/xHwzaq6HLgK+EaS\ny4E7geeq6jLgue61pLPEmuGvqiNV9Uo3/SFwENgKXA/s7hbbDdwwqyIl9e+09vmTbAOuAF4ENlfV\nke6td4HNvVYmaaYmDn+S84EngTuq6oPx96qqGN2u+1Sf25lkKcnS8vLyVMVK6s9E4U9yLqPgP1pV\nT3WzjybZ0r2/BTh2qs9W1a6qWqyqxYWFhT5qltSDSY72B3gYOFhV94+9tQe4pZu+BXim//Ikzcok\np/d+Afga8FqSfd28bwH3Ak8kuRX4EfCHsylR0iysGf6q+h6w0jWdX+m3HElD8cKeBuQer8fXz/P0\nXqlRhl9qlN3+Daru8np8rc4tv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjTL8UqM8yWcVmeE96k73\nT9fYWCmzrGsj6bPNNmKTu+WXGmX4pUbZ7T9JzfAedeNdx9NezQmf9bz9iUzZZlP9e50F3PJLjTL8\nUqMMv9Qowy81yvBLjTL8UqMMv9Qowy81yvBLjZrkXn2XJHk+yRtJXk9yezf/7iSHk+zrHtfNvlxJ\nfZnk9N6PgG9W1StJLgBeTrK3e++BqvrL2ZUnaVYmuVffEeBIN/1hkoPA1lkXJmm2TuvCniTbgCuA\nFxndvfe2JH8ELDHqHfyk7wI3qtO/nn/9n22Vbba6iQ/4JTkfeBK4o6o+AB4EPgdsZ9QzuG+Fz+1M\nspRkaXl5uYeSJfVhovAnOZdR8B+tqqcAqupoVX1cVZ8ADwE7TvXZqtpVVYtVtbiwsNBX3ZKmtGa3\nP6Pxjx4GDlbV/WPzt3THAwBuBA7MpsSNY6prwjf4teUzYZutapJ9/i8AXwNeS7Kvm/ct4OYk2xnt\nWh0Cvj6TCiXNxCRH+7/HCd+h/+/Z/suRNBTP8JMaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2qU\n4ZcaZfilRhl+qVGGX2qU4ZcaZfilRhl+qVGGX2rUaY3eqzOEI9GqB275pUYZfqlRdvvPFo4+q565\n5ZcaZfilRhl+qVGGX2qU4ZcatWb4k/xykh8keTXJ60nu6eZ/JsmLSd5K8g9Jzpt9uZL6MsmW/3+A\nL1fV5xndjvvaJFcB3wYeqKrfAH4C3Dq7MiX1bc3w18h/dy/P7R4FfBn4p27+buCGmVQoaSYm2udP\nck53h95jwF7gh8BPq+qjbpF3gK2zKVHSLEwU/qr6uKq2AxcDO4DfmnQFSXYmWUqytLy8vM4yJfXt\ntI72V9VPgeeB3wE+neT46cEXA4dX+MyuqlqsqsWFhYWpipXUn0mO9i8k+XQ3/SvANcBBRl8Cf9At\ndgvwzKyKlNS/SS7s2QLsTnIOoy+LJ6rqn5O8ATye5M+BfwcenmGdknq2Zviraj9wxSnmv81o/1/S\nWcgz/KRGGX6pUYZfapThlxpl+KVGpWq4weGSLAM/6l5uAt4bbOUrs44TWceJzrY6fr2qJjqbbtDw\nn7DiZKmqFueycuuwDuuw2y+1yvBLjZpn+HfNcd3jrONE1nGiDVvH3Pb5Jc2X3X6pUXMJf5Jrk/xn\nN/jnnfOooavjUJLXkuxLsjTgeh9JcizJgbF5FyXZm+TN7vnCOdVxd5LDXZvsS3LdAHVckuT5JG90\ng8Te3s0ftE1WqWPQNhls0NyqGvQBnMNoGLDPAucBrwKXD11HV8shYNMc1vtF4ErgwNi8vwDu7Kbv\nBL49pzruBv5k4PbYAlzZTV8A/Bdw+dBtskodg7YJo5uwn99Nnwu8CFwFPAHc1M3/G+CPp1nPPLb8\nO4C3qurtqvoZ8Dhw/RzqmJuqegF4/6TZ1zMaCBUGGhB1hToGV1VHquqVbvpDRoPFbGXgNlmljkHV\nyMwHzZ1H+LcCPx57Pc/BPwv4bpKXk+ycUw3Hba6qI930u8DmOdZyW5L93W7BzHc/xiXZxmj8iBeZ\nY5ucVAcM3CZDDJrb+gG/q6vqSuD3gW8k+eK8C4LRNz/zuyn3g8DnGN2j4Qhw31ArTnI+8CRwR1V9\nMP7ekG1yijoGb5OaYtDcSc0j/IeBS8Zerzj456xV1eHu+RjwNPMdmehoki0A3fOxeRRRVUe7/3if\nAA8xUJskOZdR4B6tqqe62YO3yanqmFebdOs+7UFzJzWP8L8EXNYduTwPuAnYM3QRST6V5ILj08BX\ngQOrf2qm9jAaCBXmOCDq8bB1bmSANkkSRmNAHqyq+8feGrRNVqpj6DYZbNDcoY5gnnQ08zpGR1J/\nCPzpnGr4LKNfGl4FXh+yDuAxRt3H/2W073Yr8KvAc8CbwL8BF82pjr8DXgP2MwrflgHquJpRl34/\nsK97XDd0m6xSx6BtAvw2o0Fx9zP6ovmzsf+zPwDeAv4R+KVp1uMZflKjWj/gJzXL8EuNMvxSowy/\n1CjDLzXK8EuNMvxSowy/1Kj/AxYxXrnleAaXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1059ea160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def box_to_rect(box, color, linewidth=3):\n",
    "    \"\"\"convert an anchor box to a matplotlib rectangle\"\"\"\n",
    "    box = box.numpy()\n",
    "    return plt.Rectangle(\n",
    "        (box[0], box[1]), (box[2]-box[0]), (box[3]-box[1]),\n",
    "        fill=False, edgecolor=color, linewidth=linewidth)\n",
    "colors = ['blue', 'green', 'red', 'black', 'magenta']\n",
    "plt.imshow(torch.ones((n, n, 3)).numpy())\n",
    "anchors = boxes[n//2, n//2, :, :]\n",
    "for i in range(anchors.shape[0]):\n",
    "    plt.gca().add_patch(box_to_rect(anchors[i,:]*n, colors[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prediction torch.Size([2, 55, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "def class_predictor(in_channel,num_anchors, num_classes):\n",
    "    \"\"\"return a layer to predict classes\"\"\"\n",
    "    return nn.Conv2d(in_channel,num_anchors * (num_classes + 1), 3, padding=1)\n",
    "\n",
    "cls_pred = class_predictor(3,5, 10)\n",
    "x = torch.randn(2, 3, 20, 20)\n",
    "print('Class prediction', cls_pred(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict anchor box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box prediction torch.Size([2, 40, 20, 20])\n"
     ]
    }
   ],
   "source": [
    "def box_predictor(in_channel,num_anchors):\n",
    "    \"\"\"return a layer to predict delta locations\"\"\"\n",
    "    return nn.Conv2d(in_channel,num_anchors * 4, 3, padding=1)\n",
    "\n",
    "box_pred = box_predictor(3,10)\n",
    "x = torch.randn(2, 3, 20, 20)\n",
    "print('Box prediction', box_pred(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Down sample features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before torch.Size([2, 3, 20, 20]) after torch.Size([2, 10, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "class down_sample(nn.Module):\n",
    "    def __init__(self, in_channel,num_filters,maxPool = True):\n",
    "        super(down_sample, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.in_ = [in_channel,num_filters]\n",
    "        self.maxPool = maxPool\n",
    "    def forward(self, x):\n",
    "        for i in range(2):\n",
    "            x = nn.Conv2d(self.in_[i], self.num_filters, 3,1,padding=1)(x)\n",
    "            x = nn.BatchNorm2d(self.num_filters)(x)\n",
    "            x = nn.ReLU(inplace=True)(x)\n",
    "        if(self.maxPool):\n",
    "            x = nn.MaxPool2d(2)(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "blk = down_sample(3,10)\n",
    "x = torch.randn(2, 3, 20, 20)\n",
    "print('Before', x.shape, 'after', blk(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manage preditions from multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_prediction(pred):\n",
    "    batch = pred.shape[0]\n",
    "    return (pred.permute(0, 2, 3, 1).contiguous().view(batch,-1))\n",
    "\n",
    "def concat_predictions(preds):\n",
    "    return torch.cat(tuple([flatten_prediction(p) for p in preds]), 1)\n",
    "    #return torch.cat(tuple(preds), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flatten class prediction 1 torch.Size([2, 22000])\n",
      "Flatten class prediction 2 torch.Size([2, 3300])\n",
      "Concat class predictions torch.Size([2, 25300])\n"
     ]
    }
   ],
   "source": [
    "y1 = torch.randn(2, 55, 20, 20)\n",
    "y2 = torch.randn(2, 33, 10, 10)\n",
    "\n",
    "flat_y1 = flatten_prediction(y1)\n",
    "print('Flatten class prediction 1', flat_y1.shape)\n",
    "flat_y2 = flatten_prediction(y2)\n",
    "print('Flatten class prediction 2', flat_y2.shape)\n",
    "print('Concat class predictions', concat_predictions([y1, y2]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Body network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body network [torch.Size([128, 32, 32]), torch.Size([128, 32, 32])]\n"
     ]
    }
   ],
   "source": [
    "def body():\n",
    "    \"\"\"return the body network\"\"\"\n",
    "    in_ = [3,16,32,64]\n",
    "    out_=[16, 32, 64,128]\n",
    "    return nn.Sequential(down_sample(in_[0],out_[0]),\n",
    "                         down_sample(in_[1],out_[1]),\n",
    "                         down_sample(in_[2],out_[2]),\n",
    "                         down_sample(in_[3],out_[3],False))\n",
    "\n",
    "bnet = body()\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "print('Body network', [y.shape for y in bnet(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a toy SSD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalAvgPool2d(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlobalAvgPool2d, self).__init__()\n",
    "    def forward(self,x):\n",
    "        return torch.mean(x.view(x.size(0), x.size(1), -1), dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blk(i):\n",
    "    if i == 0:\n",
    "        blk = body()\n",
    "    elif i == 4:\n",
    "        blk = GlobalAvgPool2d()\n",
    "    else:\n",
    "        blk = down_sample(128,128) \n",
    "    return blk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blk_forward(X, blk, size, ratio, class_predictor, box_predictor):\n",
    "    Y = blk(X)\n",
    "    print(\"out : \",Y.shape)\n",
    "    anchors = MultiBoxPrior(Y, sizes=size, ratios=ratio)\n",
    "    cls_preds = class_predictor(Y)\n",
    "    bbox_preds = box_predictor(Y)\n",
    "    return (Y, anchors, cls_preds, bbox_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All things together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79]]\n",
    "ratios = [[1, 2, 0.5]] * 4\n",
    "num_anchors = len(sizes[0]) + len(ratios[0]) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinySSD(nn.Module):\n",
    "    def __init__(self, num_classes, **kwargs):\n",
    "        super(TinySSD, self).__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        for i in range(4):\n",
    "            setattr(self, 'blk_%d' % i, get_blk(i))\n",
    "            setattr(self, 'cls_%d' % i, class_predictor(128,num_anchors,num_classes))\n",
    "            setattr(self, 'bbox_%d' % i, box_predictor(128,num_anchors))\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch = X.shape[0]\n",
    "        anchors, cls_preds, bbox_preds = [None] * 4, [None] * 4, [None] * 4\n",
    "        for i in range(4):\n",
    "            print(i,\" forward : \",X.shape)\n",
    "            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n",
    "                X, getattr(self, 'blk_%d' % i), sizes[i], ratios[i],\n",
    "                getattr(self, 'cls_%d' % i), getattr(self, 'bbox_%d' % i))\n",
    "            print(\"X : \",X.shape,\n",
    "                  \"\\nanchors : \",anchors[i].shape,\n",
    "                  \"\\ncls_preds : \",cls_preds[i].shape,\n",
    "                  \"\\nbbox_preds : \",bbox_preds[i].shape,\"\\n\")\n",
    "    \n",
    "        return (torch.cat(tuple(anchors), 1).view(1,-1,4),\n",
    "                concat_predictions(cls_preds).view(batch,-1,self.num_classes+1),\n",
    "                concat_predictions(bbox_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  forward :  torch.Size([8, 3, 256, 256])\n",
      "out :  torch.Size([8, 128, 32, 32])\n",
      "X :  torch.Size([8, 128, 32, 32]) \n",
      "anchors :  torch.Size([8, 512, 4]) \n",
      "cls_preds :  torch.Size([8, 8, 32, 32]) \n",
      "bbox_preds :  torch.Size([8, 16, 32, 32]) \n",
      "\n",
      "1  forward :  torch.Size([8, 128, 32, 32])\n",
      "out :  torch.Size([8, 128, 16, 16])\n",
      "X :  torch.Size([8, 128, 16, 16]) \n",
      "anchors :  torch.Size([8, 128, 4]) \n",
      "cls_preds :  torch.Size([8, 8, 16, 16]) \n",
      "bbox_preds :  torch.Size([8, 16, 16, 16]) \n",
      "\n",
      "2  forward :  torch.Size([8, 128, 16, 16])\n",
      "out :  torch.Size([8, 128, 8, 8])\n",
      "X :  torch.Size([8, 128, 8, 8]) \n",
      "anchors :  torch.Size([8, 32, 4]) \n",
      "cls_preds :  torch.Size([8, 8, 8, 8]) \n",
      "bbox_preds :  torch.Size([8, 16, 8, 8]) \n",
      "\n",
      "3  forward :  torch.Size([8, 128, 8, 8])\n",
      "out :  torch.Size([8, 128, 4, 4])\n",
      "X :  torch.Size([8, 128, 4, 4]) \n",
      "anchors :  torch.Size([8, 8, 4]) \n",
      "cls_preds :  torch.Size([8, 8, 4, 4]) \n",
      "bbox_preds :  torch.Size([8, 16, 4, 4]) \n",
      "\n",
      "Outputs: \n",
      " anchors torch.Size([1, 5440, 4]) \n",
      " class prediction torch.Size([8, 5440, 2]) \n",
      " box prediction torch.Size([8, 21760])\n"
     ]
    }
   ],
   "source": [
    "net = TinySSD(1)\n",
    "x = torch.randn(8, 3, 256, 256)\n",
    "default_anchors, class_predictions, box_predictions = net(x)\n",
    "print('Outputs:', '\\n anchors', default_anchors.shape, \n",
    "      '\\n class prediction', class_predictions.shape, \n",
    "      '\\n box prediction', box_predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiBoxTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from box_utils import *\n",
    "\n",
    "ground_truth = torch.tensor([[0, 0.1, 0.08, 0.52, 0.92],\n",
    "                         [1, 0.55, 0.2, 0.9, 0.88]])\n",
    "anchors = torch.tensor([[0, 0.1, 0.2, 0.3], [0.15, 0.2, 0.4, 0.4],\n",
    "                    [0.63, 0.05, 0.88, 0.98], [0.66, 0.45, 0.8, 0.8],\n",
    "                    [0.57, 0.3, 0.92, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0536, 0.0000],\n",
      "        [0.1417, 0.0000],\n",
      "        [0.0000, 0.5657],\n",
      "        [0.0000, 0.2059],\n",
      "        [0.0000, 0.7459]])\n"
     ]
    }
   ],
   "source": [
    "ious = jaccard(anchors,ground_truth[:,1:])\n",
    "print(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(values=tensor([0.1417, 0.7459]), indices=tensor([1, 4]))"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious.max(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dateset init\n"
     ]
    }
   ],
   "source": [
    "from data.voc_scut import *\n",
    "\n",
    "batch_size = 8\n",
    "dataset = VOCDetection(\"/Users/seungyoun/Desktop/mAyI/SSD/data/SCUT_HEAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinySSD(\n",
       "  (blk_0): Sequential(\n",
       "    (0): down_sample()\n",
       "    (1): down_sample()\n",
       "    (2): down_sample()\n",
       "    (3): down_sample()\n",
       "  )\n",
       "  (cls_0): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bbox_0): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (blk_1): down_sample()\n",
       "  (cls_1): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bbox_1): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (blk_2): down_sample()\n",
       "  (cls_2): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bbox_2): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (blk_3): down_sample()\n",
       "  (cls_3): Conv2d(128, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bbox_3): Conv2d(128, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.init import xavier_uniform_\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "net.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Evalutation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_loss = nn.CrossEntropyLoss()\n",
    "bbox_loss = nn.SmoothL1Loss()\n",
    "\n",
    "def calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n",
    "    cls = cls_loss(cls_preds, cls_labels)\n",
    "    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)\n",
    "    return cls + bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_out(pull_idx,batch_size=4):\n",
    "    batch = []\n",
    "    annot_ = []\n",
    "    for i in range(batch_size):\n",
    "        data = dataset.pull_item(pull_idx)\n",
    "        img,annot = data[0].permute(2,1,0),data[1]\n",
    "        batch.append(np.array(img))\n",
    "        annot_.append(np.array(annot).astype(float))\n",
    "    batch = np.array(batch)\n",
    "    batch = torch.Tensor(batch)\n",
    "    annot_ = np.array(annot_)\n",
    "    return batch,annot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # accuracy_sum, mae_sum, num_examples, num_labels\n",
    "    train_iter.reset()\n",
    "    for batch in train_iter:\n",
    "        X = batch.data[0].as_in_context(ctx)\n",
    "        Y = batch.label[0].as_in_context(ctx)\n",
    "\n",
    "        anchors, cls_preds, bbox_preds = net(X)\n",
    "        # Label the category and offset of each anchor box\n",
    "        bbox_labels, bbox_masks, cls_labels = contrib.nd.MultiBoxTarget(\n",
    "                anchors, Y, cls_preds.transpose((0, 2, 1)))\n",
    "            # Calculate the loss function using the predicted and labeled\n",
    "            # category and offset values\n",
    "            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n",
    "                          bbox_masks)\n",
    "        l.backward()\n",
    "        trainer.step(batch_size)\n",
    "        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,\n",
    "                    bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n",
    "                    bbox_labels.size)\n",
    "    cls_err, bbox_mae = 1-metric[0]/metric[1], metric[2]/metric[3]\n",
    "    animator.add(epoch+1, (cls_err, bbox_mae))\n",
    "print('class err %.2e, bbox mae %.2e' % (cls_err, bbox_mae))\n",
    "print('%.1f exampes/sec on %s'%(train_iter.num_image/timer.stop(), ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epoch):\n",
    "    loss = 0\n",
    "    loss_loc = 0\n",
    "    loss_cls = 0\n",
    "    for i in range(3000):\n",
    "        img_batch, label  = batch_out(i,2)\n",
    "        out = net(img_batch)\n",
    "        pos,pos_GT = matching(out,label)\n",
    "        print(\"is nan : \",pos,pos_GT)\n",
    "        loss_loc += smooth_l1(pos,pos_GT)\n",
    "        loss_cls += focal(sigmoid(out[1]))\n",
    "        loss = loss_loc+loss_cls\n",
    "        print(loss, loss_loc,loss_cls)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"epoch \",ep,\" \",loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
